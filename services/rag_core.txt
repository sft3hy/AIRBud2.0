== FILE / FOLDER STRUCTURE (relative to /Users/samueltownsend/dev/cosmic/Advanced-Rag-Microservices/services/rag_core) ==
./
src/
src/core/
src/utils/
Dockerfile
main.py
requirements.txt
src/core/chunking.py
src/core/data
src/core/data_models.py
src/core/llm_client.py
src/core/persistence.py
src/core/rag_pipeline.py
src/utils/db_utils.py


== CONCATENATED FILE CONTENTS ==

---- FILE: Dockerfile ----
FROM python:3.10-slim

# System deps
RUN apt-get update && apt-get install -y build-essential && rm -rf /var/lib/apt/lists/*

WORKDIR /app

COPY requirements.txt .
# faiss-cpu is universal
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

---- FILE: main.py ----
import os
import shutil
import uuid
import traceback
from typing import List, Dict
import glob
import json

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from src.core.rag_pipeline import SmartRAG
from src.utils.db_utils import DatabaseManager

# --- Configuration ---
DATA_DIR = "/app/data"
UPLOAD_DIR = os.path.join(DATA_DIR, "uploads")
CHARTS_DIR = os.path.join(DATA_DIR, "charts")

# Ensure directories exist on startup
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(CHARTS_DIR, exist_ok=True)

# --- App Initialization ---
app = FastAPI(title="Smart RAG API", version="2.0")
db = DatabaseManager()

# --- Middleware (CORS) ---
# Allows the React frontend (running on port 5173) to talk to this backend
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Static Files ---
# Mounts /app/data to /static so React can load images via URL
# e.g. http://localhost:8000/static/charts/xyz/image.png
app.mount("/static", StaticFiles(directory=DATA_DIR), name="static")
job_status: Dict[str, Dict] = {}


# --- Pydantic Models ---
class SessionCreate(BaseModel):
    filenames: List[str]


class ProcessRequest(BaseModel):
    session_id: int
    filename: str
    vision_model: str


class QueryRequest(BaseModel):
    session_id: int
    question: str


# --- Endpoints ---


@app.get("/sessions/{session_id}/charts")
def get_session_charts(session_id: int):
    """
    Scans for charts AND matches them with descriptions from the DB.
    """
    docs = db.get_session_documents(session_id)
    charts = []

    base_url = "http://localhost:8000/static"

    for doc in docs:
        chart_dir = doc.get("chart_dir")

        # 1. Parse descriptions safely
        # They might be stored as a JSON string or a dict depending on DB state
        descriptions = doc.get("chart_descriptions", {})
        if isinstance(descriptions, str):
            try:
                descriptions = json.loads(descriptions)
            except:
                descriptions = {}

        # 2. Find files
        if chart_dir and os.path.exists(chart_dir):
            search_path = os.path.join(chart_dir, "**", "*.png")
            files = glob.glob(search_path, recursive=True)

            for f in files:
                try:
                    filename = os.path.basename(f)
                    rel_path = os.path.relpath(f, "/app/data")
                    url = f"{base_url}/{rel_path}"

                    # Extract page number
                    import re

                    page_match = re.search(r"page(\d+)", filename)
                    page_num = int(page_match.group(1)) if page_match else 0

                    # 3. Lookup Description
                    # We try exact match, then match without extension
                    desc = descriptions.get(filename)
                    if not desc:
                        desc = descriptions.get(
                            os.path.splitext(filename)[0], "No description available."
                        )

                    charts.append(
                        {
                            "url": url,
                            "filename": filename,
                            "doc_name": doc.get("original_filename", "Unknown"),
                            "page": page_num,
                            "description": desc,  # <--- Added this field
                        }
                    )
                except ValueError:
                    continue

    charts.sort(key=lambda x: (x["doc_name"], x["page"]))
    return charts


@app.get("/")
def health_check():
    return {"status": "online", "service": "rag_core"}


@app.post("/upload")
async def upload_file(file: UploadFile = File(...)):
    try:
        file_location = os.path.join(UPLOAD_DIR, file.filename)
        with open(file_location, "wb+") as file_object:
            shutil.copyfileobj(file.file, file_object)
        return {"info": "File saved successfully", "path": file_location}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to upload file: {str(e)}")


@app.get("/sessions")
def get_sessions():
    """Returns list of past sessions."""
    try:
        results = db.get_all_sessions()
        # Map tuple to dict for JSON response
        return [{"id": s[0], "name": s[1], "date": s[2], "docs": s[3]} for s in results]
    except Exception as e:
        print(f"DB Error: {e}")
        return []


@app.post("/sessions")
def create_session(req: SessionCreate):
    """Creates a new session entry in the DB."""
    session_id = db.create_session(req.filenames)
    return {"session_id": session_id}


@app.get("/sessions/{session_id}/history")
def get_history(session_id: int):
    """Returns chat history for a specific session."""
    return db.get_queries_for_session(session_id)


@app.get("/sessions/{session_id}/documents")
def get_session_documents(session_id: int):
    """Returns all documents processed in this session."""
    return db.get_session_documents(session_id)


@app.get("/sessions/{session_id}/status")
def get_session_status(session_id: str):
    """Frontend polls this to get updates."""
    return job_status.get(
        str(session_id), {"status": "idle", "step": "", "progress": 0}
    )


# --- BACKGROUND WORKER ---
def run_pipeline_task(session_id: int, filename: str, vision_model: str):
    sid = str(session_id)
    try:
        # 1. Update Status
        job_status[sid] = {
            "status": "processing",
            "step": "Initializing Pipeline...",
            "progress": 10,
        }

        file_path = os.path.join(UPLOAD_DIR, filename)
        unique_folder = f"{uuid.uuid4()}_{filename}"
        output_dir = os.path.join(CHARTS_DIR, unique_folder)
        os.makedirs(output_dir, exist_ok=True)

        rag = SmartRAG(output_dir=output_dir, vision_model_name=vision_model)

        # 2. Update Status: Parsing
        job_status[sid] = {
            "status": "processing",
            "step": "Parsing Document Layout...",
            "progress": 25,
        }

        # Note: We can't easily hook into rag.index_document internals without modifying it,
        # but we can update status before/after major blocks if you broke them up.
        # For now, we assume this step takes the bulk of time.

        # 3. Update Status: Vision
        # We simulate a "step" update here. In a deeper refactor, you'd pass a callback to SmartRAG.
        # Since this call blocks, the UI will stay on "Parsing/Analyzing" for a while.
        rag.index_document(file_path)

        # 4. Update Status: Saving
        job_status[sid] = {
            "status": "processing",
            "step": "Generating Embeddings...",
            "progress": 80,
        }

        doc_id = db.add_document_record(
            filename=filename,
            vision_model=vision_model,
            chart_dir=output_dir,
            faiss_path="",
            chunks_path="",
            chart_descriptions=rag.chart_descriptions,
            session_id=session_id,
        )

        rag.save_state(doc_id)

        faiss_path = f"/app/data/faiss_indexes/index_{doc_id}.faiss"
        chunks_path = f"/app/data/chunks/chunks_{doc_id}.pkl"
        db.update_document_paths(doc_id, faiss_path, chunks_path)

        # 5. Done
        job_status[sid] = {"status": "completed", "step": "Ready!", "progress": 100}

    except Exception as e:
        traceback.print_exc()
        job_status[sid] = {"status": "error", "step": str(e), "progress": 0}


@app.post("/process")
def process_document(req: ProcessRequest, background_tasks: BackgroundTasks):
    """
    Now returns immediately and runs logic in background.
    """
    file_path = os.path.join(UPLOAD_DIR, req.filename)
    if not os.path.exists(file_path):
        raise HTTPException(status_code=404, detail="File not found")

    # Set initial status
    job_status[str(req.session_id)] = {
        "status": "queued",
        "step": "Queued for processing...",
        "progress": 5,
    }

    # Dispatch to background
    background_tasks.add_task(
        run_pipeline_task, req.session_id, req.filename, req.vision_model
    )

    return {"status": "queued", "message": "Processing started in background"}


@app.post("/query")
def query(req: QueryRequest):
    docs = db.get_session_documents(req.session_id)
    if not docs:
        return {"response": "No documents found in this session.", "results": []}

    pipelines = []
    try:
        for doc in docs:
            p = SmartRAG(output_dir=doc["chart_dir"], load_vision=False)
            if os.path.exists(doc["faiss_index_path"]) and os.path.exists(
                doc["chunks_path"]
            ):
                p.load_state(doc["faiss_index_path"], doc["chunks_path"])
                pipelines.append(p)

        if not pipelines:
            return {
                "response": "Error: Document indexes could not be loaded.",
                "results": [],
            }

        result = pipelines[0].query_multiple(req.question, pipelines)
        if "error" not in result:
            db.add_query_record(
                req.session_id, req.question, result["response"], result["results"]
            )
        return result
    except Exception as e:
        traceback.print_exc()
        return {"response": "An error occurred.", "results": [], "error": str(e)}


---- FILE: requirements.txt ----
fastapi
uvicorn
langchain
langchain-text-splitters
sentence-transformers
faiss-cpu
groq
requests
numpy
python-multipart
psycopg2-binary

---- FILE: src/core/chunking.py ----
import re
from typing import List, Tuple, Dict
from src.core.data_models import Chunk
from langchain_text_splitters import RecursiveCharacterTextSplitter
from uuid import uuid4



class DocumentChunker:
    """
    Implements the Parent-Child chunking strategy.
    Splits document into large parent chunks (for context)
    and smaller child chunks (for embedding/retrieval).
    """

    def __init__(
        self,
        child_chunk_size: int = 400,
        child_chunk_overlap: int = 50,
        parent_chunk_size: int = 2000,
        parent_chunk_overlap: int = 200,
    ):
        self.child_splitter = RecursiveCharacterTextSplitter(
            chunk_size=child_chunk_size,
            chunk_overlap=child_chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        # using Recursive here as it's generally safer than Markdown only for generic text
        self.parent_splitter = RecursiveCharacterTextSplitter(
            chunk_size=parent_chunk_size,
            chunk_overlap=parent_chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
        )

    def process(self, text: str, source: str) -> Tuple[List[Chunk], Dict[str, Chunk]]:
        """
        Returns:
            - child_chunks: List of Chunk objects (to be embedded)
            - parent_map: Dict[parent_id, Chunk] (to be retrieved)
        """
        parent_docs = self.parent_splitter.create_documents([text])

        parent_map = {}
        child_chunks = []
        if "/" in source:
            source = source.split("/")[-1]

        for p_idx, p_doc in enumerate(parent_docs):
            # Create Parent Chunk
            parent_id = str(uuid4())
            parent_chunk = Chunk(
                text=p_doc.page_content,
                source=source,
                page=0,  # Page handling would require more complex parsing logic
                chunk_id=parent_id,
                is_parent=True,
                metadata={"index": p_idx},
            )
            parent_map[parent_id] = parent_chunk

            # Create Child Chunks from this Parent
            child_docs = self.child_splitter.create_documents([p_doc.page_content])

            for c_doc in child_docs:
                child_id = str(uuid4())
                child_chunk = Chunk(
                    text=c_doc.page_content,
                    source=source,
                    page=0,
                    chunk_id=child_id,
                    parent_id=parent_id,
                    is_parent=False,
                )
                child_chunks.append(child_chunk)

        return child_chunks, parent_map




---- FILE: src/core/data ----


---- FILE: src/core/data_models.py ----
from dataclasses import dataclass
from dataclasses import field
from typing import Optional, Dict, Any

@dataclass
class Chunk:
    """
    Updated Chunk model to support Parent-Child relationships.
    """
    text: str
    source: str
    page: int
    chunk_id: str
    parent_id: Optional[str] = None
    is_parent: bool = False
    metadata: Dict[str, Any] = field(default_factory=dict)


---- FILE: src/core/llm_client.py ----
import os
import requests
from groq import Groq
from typing import List, Dict, Any
import json

class GroqClient:
    def __init__(self):
        self.client = Groq(api_key=os.environ.get("GROQ_API_KEY"))

    def create_chat_completion(self, model: str, messages: list, temperature: float, max_tokens: int):
        # Groq returns a Pydantic object
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
        )

# Helper classes to make Dict responses look like Pydantic objects
class MockMessage:
    def __init__(self, content):
        self.content = content

class MockChoice:
    def __init__(self, content):
        self.message = MockMessage(content)

class MockResponse:
    def __init__(self, content):
        self.choices = [MockChoice(content)]

class SanctuaryClient:
    def __init__(
        self,
        api_key: str = None,
        base_url: str = "https://api-sanctuary.i2cv.io",
        model_name: str = "bedrock-claude-3-5-sonnet-v1",
    ):
        # FIX: Removed super().__init__(model_name)
        self.model_name = model_name
        self.api_key = api_key or os.environ.get("SANCTUARY_API_KEY")
        self.base_url = base_url.rstrip("/")
        self.session = requests.Session()
        self.session.headers.update(
            {"Content-Type": "application/json", "Authorization": f"Bearer {self.api_key}"}
        )

    def create_chat_completion(self, model: str, messages: List[Dict], temperature: float = 0.3, max_tokens: int = 1024) -> Any:
        payload = {
            "model": self.model_name, # Sanctuary uses its own model config
            "messages": messages,
            "temperature": temperature,
        }

        try:
            response = self.session.post(
                f"{self.base_url}/v1/chat/completions", json=payload
            )
            
            if not response.ok:
                print(f"Sanctuary API Error: {response.text}")
                # Fallback or raise
                return MockResponse("Error: Could not retrieve answer from Sanctuary.")

            data = response.json()
            
            # Extract content from typical OpenAI/Sanctuary JSON format
            # Usually: {'choices': [{'message': {'content': '...'}}]}
            try:
                content = data['choices'][0]['message']['content']
                return MockResponse(content)
            except (KeyError, IndexError):
                return MockResponse(str(data))

        except requests.exceptions.RequestException as e:
            print(f"Request failed: {e}")
            return MockResponse(f"Error calling API: {e}")

---- FILE: src/core/persistence.py ----
# src/core/persistence.py

import faiss
import pickle
import os
from typing import List, Tuple
from .data_models import Chunk

FAISS_DIR = "data/faiss_indexes"
CHUNKS_DIR = "data/chunks"


def save_rag_state(
    doc_id: int, index: faiss.Index, chunks: List[Chunk]
) -> Tuple[str, str]:
    """
    Saves the FAISS index and chunks list to disk.

    Args:
        doc_id (int): The unique ID of the document session.
        index (faiss.Index): The FAISS vector index.
        chunks (List[Chunk]): The list of Chunk objects.

    Returns:
        A tuple of (faiss_path, chunks_path).
    """
    os.makedirs(FAISS_DIR, exist_ok=True)
    os.makedirs(CHUNKS_DIR, exist_ok=True)

    faiss_path = os.path.join(FAISS_DIR, f"index_{doc_id}.faiss")
    chunks_path = os.path.join(CHUNKS_DIR, f"chunks_{doc_id}.pkl")

    # Save the FAISS index
    faiss.write_index(index, faiss_path)
    print(f"✓ FAISS index saved to {faiss_path}")

    # Save the chunks list using pickle
    with open(chunks_path, "wb") as f:
        pickle.dump(chunks, f)
    print(f"✓ Chunks list saved to {chunks_path}")

    return faiss_path, chunks_path


def load_rag_state(
    faiss_path: str, chunks_path: str
) -> Tuple[faiss.Index, List[Chunk]]:
    """
    Loads the FAISS index and chunks list from disk.

    Args:
        faiss_path (str): Path to the saved FAISS index file.
        chunks_path (str): Path to the saved chunks pickle file.

    Returns:
        A tuple of (loaded_index, loaded_chunks).
    """
    if not os.path.exists(faiss_path):
        raise FileNotFoundError(f"FAISS index file not found: {faiss_path}")

    if not os.path.exists(chunks_path):
        raise FileNotFoundError(f"Chunks file not found: {chunks_path}")

    # Load the FAISS index
    index = faiss.read_index(faiss_path)
    print(f"✓ FAISS index loaded from {faiss_path}")

    # Load the chunks list
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)
    print(f"✓ Chunks list loaded from {chunks_path} ({len(chunks)} chunks)")

    return index, chunks


def delete_rag_state(doc_id: int) -> bool:
    """
    Deletes the saved FAISS index and chunks for a document.

    Args:
        doc_id (int): The unique ID of the document session.

    Returns:
        bool: True if files were deleted, False if files didn't exist.
    """
    faiss_path = os.path.join(FAISS_DIR, f"index_{doc_id}.faiss")
    chunks_path = os.path.join(CHUNKS_DIR, f"chunks_{doc_id}.pkl")

    deleted = False

    if os.path.exists(faiss_path):
        os.remove(faiss_path)
        print(f"✓ Deleted FAISS index: {faiss_path}")
        deleted = True

    if os.path.exists(chunks_path):
        os.remove(chunks_path)
        print(f"✓ Deleted chunks file: {chunks_path}")
        deleted = True

    return deleted


def get_state_size(doc_id: int) -> dict:
    """
    Gets the file sizes for a document's saved state.

    Args:
        doc_id (int): The unique ID of the document session.

    Returns:
        dict: Dictionary with 'faiss_size', 'chunks_size', and 'total_size' in bytes.
    """
    faiss_path = os.path.join(FAISS_DIR, f"index_{doc_id}.faiss")
    chunks_path = os.path.join(CHUNKS_DIR, f"chunks_{doc_id}.pkl")

    sizes = {"faiss_size": 0, "chunks_size": 0, "total_size": 0}

    if os.path.exists(faiss_path):
        sizes["faiss_size"] = os.path.getsize(faiss_path)

    if os.path.exists(chunks_path):
        sizes["chunks_size"] = os.path.getsize(chunks_path)

    sizes["total_size"] = sizes["faiss_size"] + sizes["chunks_size"]

    return sizes


def list_all_saved_states() -> List[int]:
    """
    Lists all document IDs that have saved state files.

    Returns:
        List[int]: List of document IDs with saved states.
    """
    doc_ids = set()

    # Check FAISS directory
    if os.path.exists(FAISS_DIR):
        for filename in os.listdir(FAISS_DIR):
            if filename.startswith("index_") and filename.endswith(".faiss"):
                doc_id = filename.replace("index_", "").replace(".faiss", "")
                try:
                    doc_ids.add(int(doc_id))
                except ValueError:
                    continue

    # Check chunks directory
    if os.path.exists(CHUNKS_DIR):
        for filename in os.listdir(CHUNKS_DIR):
            if filename.startswith("chunks_") and filename.endswith(".pkl"):
                doc_id = filename.replace("chunks_", "").replace(".pkl", "")
                try:
                    doc_ids.add(int(doc_id))
                except ValueError:
                    continue

    return sorted(list(doc_ids))


def cleanup_orphaned_states(valid_doc_ids: List[int]) -> int:
    """
    Removes saved state files for document IDs that are no longer in the database.

    Args:
        valid_doc_ids (List[int]): List of document IDs that should be kept.

    Returns:
        int: Number of orphaned states cleaned up.
    """
    all_saved = list_all_saved_states()
    orphaned = [doc_id for doc_id in all_saved if doc_id not in valid_doc_ids]

    cleaned = 0
    for doc_id in orphaned:
        if delete_rag_state(doc_id):
            cleaned += 1

    if cleaned > 0:
        print(f"✓ Cleaned up {cleaned} orphaned state file(s)")

    return cleaned


---- FILE: src/core/rag_pipeline.py ----
import os
import requests
import numpy as np
import faiss
import pickle
from typing import List, Dict, Tuple
from sentence_transformers import SentenceTransformer
from src.core.chunking import DocumentChunker
from src.core.persistence import save_rag_state, load_rag_state
from src.core.llm_client import GroqClient, SanctuaryClient

PARSER_API = os.environ.get("PARSER_API_URL", "http://parser:8001")
VISION_API = os.environ.get("VISION_API_URL", "http://vision:8002")

client = SanctuaryClient()
if os.environ.get("TEST") == "True":
    client = GroqClient()


class SmartRAG:
    def __init__(self, output_dir, vision_model_name="Moondream2", load_vision=False):
        self.output_dir = output_dir
        self.vision_model_name = vision_model_name
        self.client = client
        self.embedding_model = SentenceTransformer(
            "sentence-transformers/all-MiniLM-L6-v2"
        )
        self.chunker = DocumentChunker()
        self.index = None
        self.child_chunks = []
        self.parent_map = {}
        self.chart_descriptions = {}

    def index_document(self, file_path):
        print(f"Indexing {file_path}...")

        # 1. Call Parser Service
        resp = requests.post(
            f"{PARSER_API}/parse",
            json={"file_path": file_path, "output_dir": self.output_dir},
        )
        if resp.status_code != 200:
            raise Exception(f"Parser failed: {resp.text}")

        data = resp.json()
        markdown_text = data["text"]
        image_paths = data["images"]

        # 2. Call Vision Service for each image
        for img_path in image_paths:
            fname = os.path.basename(img_path)
            prompt = """Analyze the image and produce a precise, factual description of its contents.

If the image contains data (e.g., charts, graphs, tables, maps, diagrams):

Identify the type of visualization.

Transcribe all visible text exactly (titles, labels, legends, annotations).

Explicitly list each data series and enumerate all data points with their associated values and units, as shown in the image.

If values are not explicitly labeled, estimate them visually and state that they are estimates.

Preserve ordering (e.g., left to right, top to bottom).

Do not summarize trends unless after listing the full data.
Do not omit numeric values.
Do not infer information that is not visually present."""

            try:
                v_resp = requests.post(
                    f"{VISION_API}/describe",
                    json={
                        "image_path": img_path,
                        "prompt": prompt,
                        "model_name": self.vision_model_name,
                    },
                )
                desc = v_resp.json().get("description", "")
                self.chart_descriptions[fname] = desc

                # Inject description into markdown
                placeholder = f"[CHART_PLACEHOLDER:{fname}]"
                replacement = f"\n> **Visual Analysis ({fname}):**\n> {desc}\n"
                markdown_text = markdown_text.replace(placeholder, replacement)
            except Exception as e:
                print(f"Vision failed for {fname}: {e}")

        # 3. Chunking
        self.child_chunks, self.parent_map = self.chunker.process(
            markdown_text, file_path
        )

        # 4. Embedding
        texts = [c.text for c in self.child_chunks]
        embeddings = self.embedding_model.encode(texts)

        # 5. Indexing
        self.index = faiss.IndexFlatL2(384)
        self.index.add(np.array(embeddings).astype("float32"))

    def save_state(self, doc_id):
        save_rag_state(doc_id, self.index, self.child_chunks)
        with open(f"data/chunks/{doc_id}_parents.pkl", "wb") as f:
            pickle.dump(self.parent_map, f)

    def load_state(self, faiss_path, chunks_path):
        self.index, self.child_chunks = load_rag_state(faiss_path, chunks_path)
        # Infer parent path
        base = os.path.dirname(chunks_path)
        doc_id = os.path.basename(chunks_path).split("_")[1].split(".")[0]
        parent_path = os.path.join(base, f"{doc_id}_parents.pkl")
        if os.path.exists(parent_path):
            with open(parent_path, "rb") as f:
                self.parent_map = pickle.load(f)

    def search(self, query, top_k=5):
        query_emb = self.embedding_model.encode([query])
        D, I = self.index.search(np.array(query_emb).astype("float32"), top_k * 3)

        results = []
        seen_parents = set()

        for dist, idx in zip(D[0], I[0]):
            if idx < len(self.child_chunks):
                child = self.child_chunks[idx]
                if child.parent_id and child.parent_id in self.parent_map:
                    if child.parent_id not in seen_parents:
                        parent = self.parent_map[child.parent_id]
                        results.append((parent, float(dist)))
                        seen_parents.add(child.parent_id)
                if len(results) >= top_k:
                    break
        return results

    def query_multiple(self, question, pipelines, top_k=5):
        # Gather results from all docs
        all_results = []
        for p in pipelines:
            all_results.extend(p.search(question, top_k=3))

        # Sort globally by score (distance)
        all_results.sort(key=lambda x: x[1])
        top_results = all_results[:top_k]

        # Build Context
        context = ""
        for chunk, score in top_results:
            context += f"SOURCE: {chunk.source}\nCONTENT: {chunk.text}\n\n---\n\n"

        # Generate
        prompt = f"Context:\n{context}\n\nQuestion: {question}\n\nAnswer using the context provided."
        try:
            resp = self.client.create_chat_completion(
                model="meta-llama/llama-4-scout-17b-16e-instruct",  # Update model as needed
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=1024,
            )
            answer = resp.choices[0].message.content

            return {
                "response": answer,
                "results": [
                    {"text": c.text, "source": c.source, "page": c.page}
                    for c, s in top_results
                ],
            }
        except Exception as e:
            return {"error": str(e)}


---- FILE: src/utils/db_utils.py ----
import os
import json
import psycopg2
from psycopg2.extras import RealDictCursor
from datetime import datetime
import time


class DatabaseManager:
    def __init__(self, db_path=None):
        # db_path arg is kept for compatibility but ignored in favor of env vars
        self.host = os.environ.get("POSTGRES_SERVER", "postgres")
        self.user = os.environ.get("POSTGRES_USER", "rag_user")
        self.password = os.environ.get("POSTGRES_PASSWORD", "rag_password")
        self.db_name = os.environ.get("POSTGRES_DB", "rag_db")
        self.port = os.environ.get("POSTGRES_PORT", "5432")

        self.conn = self._connect()
        self._init_db()

    def _connect(self):
        """Connect with retry logic to handle container startup timing"""
        retries = 5
        while retries > 0:
            try:
                conn = psycopg2.connect(
                    host=self.host,
                    user=self.user,
                    password=self.password,
                    dbname=self.db_name,
                    port=self.port,
                )
                return conn
            except psycopg2.OperationalError as e:
                print(f"DB Connection failed, retrying... ({retries} left)")
                retries -= 1
                time.sleep(2)
        raise Exception("Could not connect to PostgreSQL database")

    def _init_db(self):
        with self.conn.cursor() as cur:
            # Postgres uses SERIAL for auto-increment and different types
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS sessions (
                    id SERIAL PRIMARY KEY,
                    session_name TEXT,
                    timestamp TIMESTAMP
                )
            """
            )

            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS documents (
                    id SERIAL PRIMARY KEY,
                    session_id INTEGER,
                    original_filename TEXT,
                    vision_model_used TEXT,
                    timestamp TIMESTAMP,
                    chart_dir TEXT,
                    faiss_index_path TEXT,
                    chunks_path TEXT,
                    chart_descriptions_json TEXT
                )
            """
            )

            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS queries (
                    id SERIAL PRIMARY KEY,
                    session_id INTEGER,
                    question TEXT,
                    response TEXT,
                    sources_json TEXT,
                    timestamp TIMESTAMP
                )
            """
            )
            self.conn.commit()

    def create_session(self, filenames):
        name = (
            filenames[0]
            if len(filenames) == 1
            else f"{filenames[0]} + {len(filenames)-1}"
        )
        with self.conn.cursor() as cur:
            # Postgres syntax: %s placeholders and RETURNING clause for ID
            cur.execute(
                "INSERT INTO sessions (session_name, timestamp) VALUES (%s, %s) RETURNING id",
                (name, datetime.now()),
            )
            session_id = cur.fetchone()[0]
            self.conn.commit()
            return session_id

    def add_document_record(
        self,
        filename,
        vision_model,
        chart_dir,
        faiss_path,
        chunks_path,
        chart_descriptions,
        session_id,
    ):
        desc_json = (
            json.dumps(chart_descriptions)
            if isinstance(chart_descriptions, dict)
            else chart_descriptions
        )

        with self.conn.cursor() as cur:
            cur.execute(
                """
                INSERT INTO documents 
                (session_id, original_filename, vision_model_used, timestamp, chart_dir, faiss_index_path, chunks_path, chart_descriptions_json)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id
                """,
                (
                    session_id,
                    filename,
                    vision_model,
                    datetime.now(),
                    chart_dir,
                    faiss_path,
                    chunks_path,
                    desc_json,
                ),
            )
            doc_id = cur.fetchone()[0]
            self.conn.commit()
            return doc_id

    def update_document_paths(self, doc_id, faiss_path, chunks_path):
        with self.conn.cursor() as cur:
            cur.execute(
                "UPDATE documents SET faiss_index_path=%s, chunks_path=%s WHERE id=%s",
                (faiss_path, chunks_path, doc_id),
            )
            self.conn.commit()

    def add_query_record(self, session_id, question, response, sources):
        with self.conn.cursor() as cur:
            cur.execute(
                "INSERT INTO queries (session_id, question, response, sources_json, timestamp) VALUES (%s, %s, %s, %s, %s)",
                (session_id, question, response, json.dumps(sources), datetime.now()),
            )
            self.conn.commit()

    def get_all_sessions(self):
        with self.conn.cursor() as cur:
            cur.execute(
                """
                SELECT s.id, s.session_name, s.timestamp, COUNT(d.id) 
                FROM sessions s 
                LEFT JOIN documents d ON s.id=d.session_id 
                GROUP BY s.id, s.session_name, s.timestamp 
                ORDER BY s.timestamp DESC
            """
            )
            return cur.fetchall()

    def get_session_documents(self, session_id):
        # Use RealDictCursor to return dictionary-like objects automatically
        # This keeps compatibility with the main.py logic expecting column names
        with self.conn.cursor(cursor_factory=RealDictCursor) as cur:
            cur.execute("SELECT * FROM documents WHERE session_id=%s", (session_id,))
            rows = cur.fetchall()

            results = []
            for row in rows:
                doc = dict(row)  # Convert to standard dict

                # PARSING LOGIC: Ensure descriptions are a dictionary
                raw_desc = doc.get("chart_descriptions_json", "{}")
                if not raw_desc:
                    raw_desc = "{}"

                try:
                    if isinstance(raw_desc, str):
                        doc["chart_descriptions"] = json.loads(raw_desc)
                    else:
                        doc["chart_descriptions"] = raw_desc
                except Exception as e:
                    print(f"Error parsing JSON for doc {doc.get('id')}: {e}")
                    doc["chart_descriptions"] = {}

                results.append(doc)

            return results

    def get_queries_for_session(self, session_id):
        with self.conn.cursor() as cur:
            cur.execute(
                "SELECT question, response, sources_json FROM queries WHERE session_id=%s ORDER BY timestamp ASC",
                (session_id,),
            )
            return [
                {"question": r[0], "response": r[1], "sources": json.loads(r[2])}
                for r in cur.fetchall()
            ]


