services:
  # 0. Init Filesystem (Utility Container)
  init_fs:
    image: alpine:latest
    container_name: smart_rag_init_fs
    # Setup shared data directories with permissions
    command: sh -c "mkdir -p /data/uploads /data/charts /data/faiss_indexes /data/chunks /data/huggingface /data/previews && chmod -R 777 /data"
    volumes:
      - shared_data:/data
    restart: "no" # Only runs once per deploy

  # --- Gateway ---
  nginx:
    image: nginx:alpine
    container_name: smart_rag_nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - ./nginx/html:/usr/share/nginx/html:ro
    depends_on:
      - frontend
      - rag_core
      - kg_service
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "3"

  # 1. Frontend
  frontend:
    # We keep the build context so you can run 'docker compose build'
    build:
      context: ./services/frontend
      dockerfile: Dockerfile.test
      args:
        - VITE_API_URL=http://rag_core:8000
        - VITE_NETWORK=LOW
    image: smart_rag_frontend:latest
    container_name: smart_rag_frontend
    restart: always
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://rag_core:8000
      - VITE_NETWORK=LOW
    # Assuming your Dockerfile CMD handles the static serve (e.g., serve -s dist)
    depends_on:
      - rag_core
    volumes:
      - ./services/frontend:/app
      - /app/node_modules

  # 2. RAG Core
  rag_core:
    build: ./services/rag_core
    image: smart_rag_core:latest
    container_name: smart_rag_core
    restart: always
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SANCTUARY_API_KEY=${SANCTUARY_API_KEY}
      - TEST=False
      - LLM_PROVIDER=groq
      - PARSER_API_URL=http://parser:8001
      - VISION_API_URL=http://vision:8002
      - PYTHONUNBUFFERED=1
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=rag_user
      - POSTGRES_PASSWORD=rag_password
      - POSTGRES_DB=rag_db
      - HF_HOME=/data/huggingface
    volumes:
      - shared_data:/data
    depends_on:
      parser:
        condition: service_started
      vision:
        condition: service_started
      postgres:
        condition: service_healthy
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 3. Parser
  parser:
    build:
      context: ./services/parser
      dockerfile: Dockerfile.test # Switched from Dockerfile.test to standard
    image: smart_rag_parser:latest
    container_name: smart_rag_parser
    restart: always
    ports:
      - "8001:8001"
    environment:
      - PYTHONUNBUFFERED=1
    volumes:
      - shared_data:/data
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 4. Vision
  vision:
    build: ./services/vision
    image: smart_rag_vision:latest
    container_name: smart_rag_vision
    restart: always
    ports:
      - "8002:8002"
    environment:
      - PYTHONUNBUFFERED=1
      # Ensure this points to your external Ollama instance
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
    volumes:
      - shared_data:/data
      - huggingface_cache:/root/.cache/huggingface
    extra_hosts:
      - "host.docker.internal:host-gateway"
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 5. Postgres
  postgres:
    image: postgres:15-alpine
    container_name: smart_rag_postgres
    restart: always
    environment:
      - POSTGRES_USER=rag_user
      - POSTGRES_PASSWORD=rag_password
      - POSTGRES_DB=rag_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U rag_user -d rag_db" ]
      interval: 5s
      timeout: 5s
      retries: 5

  # 6. PGAdmin (Optional for Prod - Consider removing if not strictly needed)
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: admin@example.com
      PGADMIN_DEFAULT_PASSWORD: admin
      PGADMIN_CONFIG_CONSOLE_LOG_LEVEL: 10 # Reduced log level
    ports:
      - "5050:80"
    depends_on:
      - postgres
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # 7. KG Service
  kg_service:
    build: ./services/kg_service
    image: smart_rag_kg:latest
    container_name: smart_rag_kg
    restart: always
    ports:
      - "8003:8003"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=smartrag_password
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SANCTUARY_API_KEY=${SANCTUARY_API_KEY}
      - TEST=False
      - PYTHONUNBUFFERED=1
    depends_on:
      neo4j:
        condition: service_healthy
    # Removed --reload, kept uvicorn. If you have a Gunicorn Dockerfile here too, remove this command line.
    command: uvicorn main:app --host 0.0.0.0 --port 8003

  # 8. Neo4j
  neo4j:
    image: neo4j:5.15-community
    container_name: smart_rag_neo4j
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/smartrag_password
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - neo4j_data:/data
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5

  # 9. Monitoring: InfluxDB
  influxdb:
    image: influxdb:2.7
    container_name: smart_rag_influxdb
    restart: always
    ports:
      - "8086:8086"
    volumes:
      - influxdb_data:/var/lib/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=admin
      - DOCKER_INFLUXDB_INIT_PASSWORD=smartrag_password
      - DOCKER_INFLUXDB_INIT_ORG=smartrag_org
      - DOCKER_INFLUXDB_INIT_BUCKET=glances
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=smartrag-admin-token
    healthcheck:
      test: "curl -f http://localhost:8086/health || exit 1"
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 20s

  # 10. Monitoring: Glances
  glances:
    image: nicolargo/glances:latest-full
    container_name: smart_rag_monitor
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./monitoring/glances/glances.conf:/glances/conf/glances.conf:ro
    environment:
      - DOCKER_HOST=unix://var/run/docker.sock
      - PYTHONUNBUFFERED=1
    command: /bin/sh -c "glances -C /glances/conf/glances.conf -q --export influxdb2"
    privileged: true
    pid: host
    depends_on:
      influxdb:
        condition: service_healthy

  # 11. Monitoring: Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: smart_rag_grafana
    restart: always
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=smartrag
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - influxdb

  # 12. Backup service
  backups:
    build: ./services/backup
    image: smart_rag_backups:latest
    container_name: smart_rag_backups
    restart: unless-stopped
    environment:
      - TZ=America/New_York
      - BACKUP_SCHEDULE=0 0 * * 3
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=rag_user
      - POSTGRES_PASSWORD=rag_password
      - POSTGRES_DB=rag_db
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - shared_data:/shared_data:ro
      - shared_data:/backups
      - neo4j_data:/neo4j_data:ro
      # Removed source code mount, not needed for backups usually
    depends_on:
      - postgres
      - neo4j

volumes:
  pgadmin_data:
  shared_data:
  huggingface_cache:
  postgres_data:
  neo4j_data:
  influxdb_data:
  grafana_data:
