services:
  # 0. Init Filesystem (Utility Container)
  init_fs:
    image: alpine:latest
    container_name: airbud_init_fs
    # Setup shared data directories with permissions
    command: sh -c "mkdir -p /data/uploads /data/charts /data/faiss_indexes /data/chunks /data/huggingface /data/previews && chmod -R 777 /data"
    volumes:
      - shared_data:/data
    networks:
      - airbud_network
    restart: "no" # Only runs once per deploy

  # --- Gateway ---
  nginx:
    image: nginx:alpine
    container_name: airbud_nginx
    restart: always
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/certs:/etc/nginx/certs:ro
      - ./nginx/html:/usr/share/nginx/html:ro
    depends_on:
      - frontend
      - rag_core
      - kg_service
    networks:
      - airbud_network
    logging:
      driver: "json-file"
      options:
        max-size: "500m"
        max-file: "3"

  # 1. Frontend
  frontend:
    # We keep the build context so you can run 'docker compose build'
    build:
      context: ./services/frontend
      dockerfile: Dockerfile.local
      args:
        - VITE_API_URL=https://localhost/airbud/api
        - VITE_NETWORK=LOW
    image: airbud_frontend:latest
    container_name: airbud_frontend
    restart: always
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=https://localhost/airbud/api
      - VITE_NETWORK=LOW
    # Assuming your Dockerfile CMD handles the static serve (e.g., serve -s dist)
    depends_on:
      - rag_core
    volumes:
      - ./services/frontend:/app
      - /app/node_modules
    networks:
      - airbud_network

  # 2. RAG Core
  rag_core:
    build:
      context: ./services/rag_core
      dockerfile: Dockerfile.offline
    image: airbud_core:latest
    container_name: airbud_core
    restart: always
    environment:
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SANCTUARY_API_KEY=${SANCTUARY_API_KEY}
      - TEST=False
      - LLM_PROVIDER=groq
      - PARSER_API_URL=http://parser:8001
      - VISION_API_URL=http://vision:8002
      - PYTHONUNBUFFERED=1
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=slammy
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=rag_db
      - AUTH_MODE=CAC
      - KG_SERVICE_URL=http://kg_service:8003
      # Offline Config
      - TRANSFORMERS_OFFLINE=1
      - EMBEDDING_MODEL_PATH=/models/huggingface/all-MiniLM-L6-v2
    volumes:
      - shared_data:/data
      - ./offline_models/huggingface:/models/huggingface
    depends_on:
      parser:
        condition: service_started
      vision:
        condition: service_started
      postgres:
        condition: service_healthy
    networks:
      - airbud_network
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 3. Parser
  parser:
    build:
      context: ./services/parser
      dockerfile: Dockerfile.offline
    image: airbud_parser:latest
    container_name: airbud_parser
    restart: always
    ports:
      - "8001:8001"
    environment:
      - PYTHONUNBUFFERED=1
      - OFFLINE_MODEL_PATH=/models/detectron2/publaynet_faster_rcnn_R_50_FPN_3x.pth
    volumes:
      - shared_data:/data
      - ./offline_models/detectron2:/models/detectron2
    networks:
      - airbud_network
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 4. Vision
  vision:
    build:
      context: ./services/vision
      dockerfile: Dockerfile.offline
    image: airbud_vision:latest
    container_name: airbud_vision
    restart: always
    ports:
      - "8002:8002"
    environment:
      - PYTHONUNBUFFERED=1
      # Ensure this points to your external Ollama instance
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - WHISPER_MODEL_PATH=/models/whisper/medium.pt
    volumes:
      - shared_data:/data
      - ./offline_models/whisper:/models/whisper
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - airbud_network
    # Command removed: Using CMD from Dockerfile (Gunicorn)

    # 5. Postgres
  postgres:
    image: postgres:15-alpine
    container_name: airbud_postgres
    restart: always
    environment:
      - POSTGRES_USER=slammy
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=rag_db
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U slammy -d rag_db" ]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - airbud_network

  # 6. PGAdmin (Optional for Prod - Consider removing if not strictly needed)
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin
    restart: unless-stopped
    environment:
      PGADMIN_DEFAULT_EMAIL: slammy@airbud.com
      PGADMIN_DEFAULT_PASSWORD: ${PGADMIN_PASSWORD}
      PGADMIN_CONFIG_CONSOLE_LOG_LEVEL: 10 # Reduced log level
    ports:
      - "5050:80"
    depends_on:
      - postgres
    volumes:
      - pgadmin_data:/var/lib/pgadmin
    networks:
      - airbud_network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # 7. KG Service
  kg_service:
    build:
      context: ./services/kg_service
      dockerfile: Dockerfile.local
    image: airbud_kg:latest
    container_name: airbud_kg
    restart: always
    ports:
      - "8003:8003"
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - SANCTUARY_API_KEY=${SANCTUARY_API_KEY}
      - TEST=False
      - PYTHONUNBUFFERED=1
    depends_on:
      neo4j:
        condition: service_healthy
    # Removed --reload, kept uvicorn. If you have a Gunicorn Dockerfile here too, remove this command line.
    command: uvicorn main:app --host 0.0.0.0 --port 8003
    networks:
      - airbud_network

  # 8. Neo4j
  neo4j:
    image: neo4j:5.15-community
    container_name: airbud_neo4j
    restart: always
    ports:
      - "7474:7474"
      - "7687:7687"
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD}
      - NEO4J_dbms_memory_heap_initial__size=1G
      - NEO4J_dbms_memory_heap_max__size=2G
    volumes:
      - neo4j_data:/data
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider localhost:7474 || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - airbud_network

  # 9. Monitoring: InfluxDB
  influxdb:
    image: influxdb:2.7
    container_name: airbud_influxdb
    restart: always
    ports:
      - "8086:8086"
    volumes:
      - influxdb_data:/var/lib/influxdb2
    environment:
      - DOCKER_INFLUXDB_INIT_MODE=setup
      - DOCKER_INFLUXDB_INIT_USERNAME=slammy
      - DOCKER_INFLUXDB_INIT_PASSWORD=${INFLUXDB_PASSWORD}
      - DOCKER_INFLUXDB_INIT_ORG=airbud_org
      - DOCKER_INFLUXDB_INIT_BUCKET=glances
      - DOCKER_INFLUXDB_INIT_ADMIN_TOKEN=airbud-admin-token
    healthcheck:
      test: "curl -f http://localhost:8086/health || exit 1"
      interval: 5s
      timeout: 10s
      retries: 5
      start_period: 20s
    networks:
      - airbud_network

  # 10. Monitoring: Glances
  glances:
    image: nicolargo/glances:latest-full
    container_name: airbud_monitor
    restart: unless-stopped
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./monitoring/glances/glances.conf:/glances/conf/glances.conf:ro
    environment:
      - DOCKER_HOST=unix://var/run/docker.sock
      - PYTHONUNBUFFERED=1
    command: /bin/sh -c "glances -C /glances/conf/glances.conf -q --export influxdb2"
    privileged: true
    pid: host
    depends_on:
      influxdb:
        condition: service_healthy
    networks:
      - airbud_network

  # 11. Monitoring: Grafana
  grafana:
    image: grafana/grafana:latest
    container_name: airbud_grafana
    restart: always
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=slammy
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - influxdb
    networks:
      - airbud_network

  # 12. Backup service
  backups:
    build:
      context: ./services/backup
      dockerfile: Dockerfile.local
    image: airbud_backups:latest
    container_name: airbud_backups
    restart: unless-stopped
    environment:
      - TZ=America/New_York
      - BACKUP_SCHEDULE=0 0 * * 3
      - POSTGRES_SERVER=postgres
      - POSTGRES_USER=slammy
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=rag_db
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - shared_data:/shared_data:ro
      - shared_data:/backups
      - neo4j_data:/neo4j_data:ro
      # Removed source code mount, not needed for backups usually
    depends_on:
      - postgres
      - neo4j
    networks:
      - airbud_network

volumes:
  pgadmin_data:
  shared_data:
  huggingface_cache:
  postgres_data:
  neo4j_data:
  influxdb_data:
  grafana_data:


networks:
  airbud_network:
    driver: bridge
